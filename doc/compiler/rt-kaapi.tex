%%
%% This is file `squelette-rr.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% RR.dtx  (with options: `sample')
%% ********************************************************************
%% Copyright (C) 1997-1999 2004 2006-2010 INRIA/APICS by Jose' Grimm
%% This file may be distributed and/or modified under the
%% conditions of the LaTeX Project Public License, either version 1.3
%% of this license or (at your option) any later version.
%% The latest version of this license is in
%%    http://www.latex-project.org/lppl.txt
%% and version 1.3 or later is part of all distributions of LaTeX
%% version 2003/12/01 or later.
%% An archive of the software can be found at
%%    ftp://ftp-sop.inria.fr/apics/rr-inria

\documentclass[a4paper]{article}
\usepackage[latin1]{inputenc} % ou \usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc} % ou \usepackage[OT1]{fontenc}
\usepackage{RR,RRthemes}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{color}
%%\usepackage[frenchb]{babel} % optionnel
%%
%% date de publication du rapport
\RRdate{Juin 2011}
%%
%% Cas d'une version deux
%% \RRversion{2}
%% date de publication de la version 2
%% \RRdater{Novembre  2006}
\usepackage{listings}

\usepackage{amssymb}
\usepackage{xspace} 
\usepackage{array} 
\usepackage{multirow}
\newcommand\hyph{\nobreak\hskip0pt-\nobreak\hskip0pt\relax}
\newlength\savedwidth
\newcommand\whline{\noalign{\global\savedwidth
  \arrayrulewidth\global\arrayrulewidth 1.5pt}
  \hline \noalign{\global\arrayrulewidth
  \savedwidth}
}
\newcolumntype{I}{!{\vrule width 1.5pt}}
\renewcommand{\arraystretch}{1.5}

\newcommand{\kaapi}{\textsc{X-Kaapi}\xspace}



%%
\RRauthor{% les auteurs
 % Premier auteur, avec une note
Fabien Le Mentec%
  % note partag\'ee (optionnelle)
%  \thanks[sfn]{Shared foot note}%
 % \and entre chaque auteur s'il y en a plusieurs
  \and
Thierry Gautier%\thanks{Footnote for second author}%
 % r\'ef\'erence \`a la note partag\'ee
%\thanksref{sfn}
  \and
Vincent Danjean%\thanks{Footnote for second author}%
 % r\'ef\'erence \`a la note partag\'ee
%\thanksref{sfn}
}
%%
%% Ceci apparait sur chaque page paire.
\authorhead{Le Mentec \& Gautier & Danjean}
%%
\RRtitle{The \kaapi's programming model and user's manual}
%% English title
\RRetitle{The \kaapi's programming model \\and \\User's manual}
%%
\titlehead{The \kaapi's programming model}
%%
%\RRnote{This is a note}
%\RRnote{This is a second note}
%%
\RRresume{
}
\RRabstract{During the past years, dynamically scheduled data flow graph for tiled algorithms in linear algebra appears as a promising way to reach the portability of performances onto several class  of multicore architectures.
The idea was to describe these algorithms into an abstract data flow graph representation that only encodes tasks and their dependencies independently of the scheduling algorithm, which will fold up tasks onto the cores of the concrete architecture. 
Reported experimental timings show good performances for linear algebra algorithms where tasks are generated only by nested loops. Nevertheless, all the recently proposed framework does not consider program where tasks with data flow dependencies may be generated recursively. 

In this report, we present the \kaapi's programming model. A parallel \kaapi program is a C or C++ sequential program with code annotation using \texttt{\#pragma} compiler directive. Then a specific source to source compiler translates \kaapi directives to call to the runtime.
}

%%
\RRmotcle{}
\RRkeyword{parallel computing, data flow graph, locality guided work stealing, \kaapi}
%%
%% \RRprojet{Apics}  % cas d'un seul projet
\RRprojets{MOAIS}
\RRdomaine{3} % cas du domaine numero 1
\RRthemeProj{moais} % theme du projet Apics
%\RRdomaineProjBis{apics} % domaine du projet Apics
%% \RRdomaineProjBis{pop art} % domaine du projet PopArt
%%
%% \URLorraine % pour ceux qui sont \`a l'est
%% \URRennes  % pour ceux qui sont \`a l'ouest
%% \URRhoneAlpes % pour ceux qui sont dans les montagnes
%% \URRocq % pour ceux qui sont au centre de la France
%% \URFuturs % pour ceux qui sont dans le virtuel
%% \URSophia % pour ceux qui sont au Sud.
%%
%% \RCBordeaux % centre de recherche Bordeaux - Sud Ouest
%% \RCLille % centre de recherche Lille Nord Europe
%% \RCParis % Paris Rocquencourt
%% \RCSaclay % Saclay \^Ile de France
\RCGrenoble % Grenoble - Rh\^one-Alpes
%% \RCNancy % Nancy - Grand Est
%% \RCRennes % Rennes - Bretagne Atlantique
%\RCSophia % Sophia Antipolis M\'editerran\'ee

%%
\begin{document}
%%
%\makeRR   % cas d'un rapport de recherche
\makeRT % cas d'un rapport technique.
%% a partir d'ici, chacun fait comme il le souhaite


\tableofcontents


\newpage
\section{Context}

Several research projects~\cite{Gunnels:2001:FFL,plasma-analysis,PerezBL08} have investigated the use of data flow graph as an intermediate representation of the computation of LAPACK's algebra algorithms~\cite{plasma-lapack}: the main reason was that the portability of performances in LAPACK is of the responsibility of a set of basic linear algebra subprograms (BLAS) which exhibits only low level parallelism that is not enough on multicores.

Without taking care of consideration about  tile algorithms to reach performances, three majors points have been identified as important to perform efficiently tile algorithms~\cite{Gunnels:2001:FFL,plasma-analysis,plasma-lapack}: 1/ fine granularity to reach high level of parallelism; 2/ asynchronous execution to prevent barrier of synchronization; and 3/ a dynamic data driven scheduler to ensure execution of task as soon as all their input data are produced.

Point 1/ was already mentioned since the first papers about Cilk~\cite{Cilk5:1998} and formalized in the Cilk performance model~\cite{CilkSched98} where the parallel time is lower bounded by the critical path: a program that cannot exploit fine grain parallelism is difficult to schedule with guarantee linear speed up, even for reasonable number of processors.
Points 2/  and 3/ are at the basis of the execution of data flow machine~\cite{dataflow} or language~\cite{Jade:1998, Athapascan98} that tries to schedule instruction as soon as input operand was produced.


Nevertheless, most of the recently proposed frameworks that shared the data flow graph as a central representation to schedule dynamically tasks according to their dependencies, cannot be able to perform programs with recursive creation of tasks.  

This report presents \kaapi a runtime library and a source to source compiler to program NUMA multicore machine.
\kaapi is based on a macro data flow graph: the sequential code is annotated to tell what function will be transformed into task. Each function that is candidate to become a task must be annotated in order to specify the access mode (read, write, reduction, exclusive) made through its parameter to the memory. The compiler will insert task creation and the runtime will detect the dependencies and schedule the tasks onto the cores.

\kaapi programming model comes from Athapascan~\cite{Athapascan98} which, itself, was inspired by Jade~\cite{Jade:1998} and Cilk~\cite{Cilk5:1998}. Nevertheless, because \kaapi programming model and  SMPSs was very closed, we also accept the execution of  SMPSs program.

This technical report is organized as following. Section~\ref{sec:helloworld} presents 
though simple examples how to program with \kaapi in C or C++.

%%%
%%%
\newpage
\section{Installation of the software}\label{sec:userinstall}

\kaapi is a both a programming model and a runtime for high performance parallelism targeting multicore and distributed architectures. 
It relies on work stealing paradigm.
\kaapi was developed in the MOAIS INRIA project by Thierry Gautier, Fabien Le Mentec, Vincent Danjean and Christophe Laferrière in the early stage of the library.

In this report, only the programming model based of code annotation with \texttt{pragma} compiler directive is presented.
But the runtime library comes also with a full set of complementary library programming interfaces: a C, C++ and STL like interface. The C++ and STL interfaces, at higher level than the C interface,  may be directly used for developing parallel program or library.

\subsubsection*{Supported Platforms}
\kaapi targets essentially SMP and NUMA platforms. The runtime should run
on every system providing:
\begin{itemize}
\item a GNU toolchain (4.3),
\item the pthread library,
\item Unix based environment.
\end{itemize}
It has been extensively tested on the following operating systems:
GNU-Linux/x86\_64 and MacOSX/Intel processor.
There is no version for Windows yet.

\subsubsection*{\kaapi Contacts}
If you whish to contact the XKaapi team you will  hopefully visit the web site at:
\begin{center}
\url{http://kaapi.gforge.inria.fr}
\end{center}


\subsection{Installation}

To install \kaapi programming model and compiler you need:
\begin{itemize}
\item install the \kaapi libraries and runtime
\item install the compiler
\end{itemize}
The \kaapi libraries and runtime are available on \url{http://kaapi.gforce.inria.fr} (tarball) or as debian package. \kaapi libraries and runtime is distributed with a CeCILL-C license\footnote{\url{http://www.cecill.info/index.en.html} for english version}:
\begin{center}
\begin{minipage}{0.9\linewidth}
\it
CeCILL-C is well suited to libraries and more generally software components. Anyone distributing an application which includes components under the CeCILL-C license must mention this fact and make any changes to the source code of these components available to the community under CECILL-C while being free to choose the licence of its application.\end{minipage}
\end{center}

Moreover, the \kaapi compiler is based on the ROSE\footnote{\url{http://www.rosecompiler.org}} framework to develop source-to-source translators. In this framework, C and C++ frontend is based on the EDG frontend. 
ROSE is available with a BSD license, but the binary code for EDG must be downloaded separately. 

\subsubsection{ROSE and EDG frontend installation}
ROSE framework (\url{http://www.rosecompiler.org/}) must be installed. Please have a look at the installation guide on the ROSE web site.

In order to install ROSE, you need to have (valid for ROSE 0.9.5a):
\begin{itemize}
\item \texttt{wget}: to download binary version of the EDG frontend
\item \texttt{gcc, g++}: a least version 4.0.0
\item \texttt{boost}: version 1.36.0 to 1.45.0
\item and a lot of different utilities (GNU autotools, ...)
\end{itemize}
Precise definition of the procedure to install rose and EDG frontend is describe in the ROSE documents.

Once ROSE installed, you are able to use it to generate \verb+kacc+ the \kaapi compiler.

\subsubsection{\kaapi library installation}
There are 2 ways to install \kaapi:
\begin{itemize}
\item using the debian packages,
\item installing from source.
\end{itemize}

%% sub
\subsubsection*{Using the debian packages}
Below is a list of the Debian packages provided for using and programming with \kaapi. \\
\textit{
TODO: décrire ici le package nécessaire pour le compilateur. A priori uniquement la lib C + éventuellement une lib C++ pour des classes tableaux et/ou autre data structure qui serait déjà 'pré-définie' pour être passer correctement en paramètre de tâche. Je pense ici à la lib des array/range2D qui permet facilement de faire de la découpe récursive de données.
}
%% sub
\subsubsection*{Installing from sources}

There are 2 ways to retrieve the sources:
\begin{itemize}
\item download a release snapshot at the following url:\newline
\url{https://gforge.inria.fr/frs/?group_id=94}.
\item clone the project git repository:\newline
\verb+> git clone git://git.ligforge.imag.fr/git/kaapi/xkaapi.git xkaapi+
\end{itemize}
\textit{
TODO: GIT anonymous ou non ? A préciser comment rejoindre le projet si volonté.
}


\paragraph{Configuration.}
The build system uses GNU Autotools.
In case you cloned the project repository, you first have to bootstrap the
configuration process by running the following script:
\begin{verbatim}
$> ./bootstrap
\end{verbatim}
The \textit{configure} file should be present. It is used to create the
\textit{Makefile} accordingly to your system configuration. Command line
options can be used to modify the default behavior. You can have a complete
list of the available options by running:
\begin{verbatim}
$> ./configure --help
\end{verbatim}


Below is a list of the most important ones:
\begin{itemize} %% option list
\item \verb+--enable-mode=debug+ or \verb+release+\newline
Choose the compilation mode of the library. Defaults to release.
\item \verb+--with-perfcounter+\newline
Enable performance counters support.
\item \verb+--with-papi+\newline
Enable the PAPI library for low level performance counting.
More information on PAPI can be found at http://icl.cs.utk.edu/papi/.
\item \verb+--enable-kacc+\newline \textbf{required} to compile the \verb+kacc+ compiler.
Depending of your installation of ROSE and Boost library, you may specified their installation paths using:\\
\hspace*{4ex}\verb+--with-rose=<rose installation>+ \\
or\\
\hspace*{4ex} \verb+--with-boost=<boost installation>+
\item \verb+--prefix=+\newline
Overload the default installation path.
\end{itemize} %% option list
Example:

\begin{verbatim}
./configure --enable-mode=release --prefix=$HOME/install
\end{verbatim}
If there are errors during the configuration process, you have to solve
them before going further. It is likely there is a missing dependency on your
system, in which case the log gives you the name of the software to install.

\paragraph{Compilation and installation.}
On success, the configuration process generates a Makefile. the 2 following
commands build and install the \kaapi runtime:
\begin{verbatim}
$> make
$> make install
\end{verbatim}

\paragraph{Checking the installation.}
The following checks the runtime is correctly installed on your system:
\begin{verbatim}
$> make check
\end{verbatim}

\paragraph{Compilation of the examples.}
The following compiles the sample applications:
\begin{verbatim}
$> cd examples; make examples
\end{verbatim}

\subsubsection*{Installation directory}

The configure, make, make install, commands create in the prefix directory the following 
directory structure:
\begin{verbatim}
<prefiix>/include
<prefiix>/lib
<prefiix>/bin
<prefiix>/share
\end{verbatim}

The KaCC compiler is located in \texttt{<prefix>/bin}, it should be in your \texttt{\$PATH} variable to avoid full path naming.
All programs compiled with KaCC are linked against libraries located in  \texttt{<prefix>/lib}. 
Executable linked with KaCC does not required to add this directory in your \verb+LD_LIBRARY_PATH+ (under linux) or \verb+DYLD_LIBRARY_PATH+ (under Mac OS X).

The \texttt{<prefix>/share} directory contains some documentation and examples.

\newpage\section{Getting started}\label{sec:helloworld}

The source distribution of \kaapi contains the repository \verb+examples+. Each subdirectory is dedicated to a problem (fibonacci, matrix product, ...). 
The examples that comes with the compiler are located in \verb+examples/compiler+.
Presentation of examples using directly the \kaapi library (with C, C++ or STL) interface is out of the scope of this report.

In the remainder, we only present the \kaapi programming model and we show examples through to use the \verb+kacc+ compiler only. The process of describing parallel program with \kaapi is:
\begin{itemize}
\item \kaapi is task based parallel programming model: the user should indicates where is task.
\item A task is a function call without side effect: the user must annotate the code to specify what function is a task. 
\item The compiler will insert task creation code each time at each instruction where function call to a task is found.
\item Dependencies between tasks are automatically computed by the runtime thanks the data flow dependencies between function calls: in the same time a function is annotated as task, the user must specify the access mode to the memory made for each formal parameters. 
\end{itemize}
All the annotations are specified by pragma directive with the form:
\begin{center}
\texttt{\#pragma kaapi <clause specification>}
\end{center}

\subsection{Hello World}
The example of figure~\ref{ex:helloworld} presents the creation of one task that display "Hello world".
\kaapi \verb+kacc+ compiler is based on pragma annotation, as for OpenMP, to describe the parallelism.

First it is necessary to start a parallel region where task may be executed concurrently.
Outside parallel region task is executed only by the current thread. 
This is indicated by \texttt{\#pragma kaapi parallel} at line 11. 
The parallel region begins at the next basic block (between \verb+{+ and \verb+}+) and it ends at the end of the basic block. It could also be define before any instruction and it will end after the instruction.

The \texttt{\#pragma kaapi parallel} defines a region where concurrent execution may occurs. The number of threads used to execute tasks inside parallel region may varies during the execution. 

\begin{figure}[ht]
\hrule\vspace*{2ex}
\begin{center}
\begin{minipage}{0.9\linewidth}
\begin{verbatim}
1  #include <stdio.h>
2  
3  #pragma kaapi task value(msg)
4  void say_hello(const char* msg)
5  {
6    printf("%s\n", msg);
7  }
8
9  int main(int argc, char** argv)
10 {
11 #pragma kaapi parallel
12   say_hello("Hello World!");
13   return 0;
14 }
\end{verbatim}
\end{minipage}
\end{center}
\hrule
\caption{hello\_world1.c example}\label{ex:helloworld}
\end{figure}
The task in the figure takes one parameter (a C string) and displays it to the screen. The parameter is passed by value: this is specified by the clause \texttt{value(msg)} that tells to the compiler that the effective parameter is considered as passing by value. We will see in the following sections exactly what is means.

All tasks created into a parallel region are guaranteed to be completed at the end of the parallel region.

\subsubsection{Invoking the compiler}
To compile the previous example simple enter:
\begin{verbatim}
> kacc -o hello_world hello_world.c
\end{verbatim}
An to run it:
\begin{verbatim}
> ./hello_world
Hello World!
\end{verbatim}

\subsubsection{Two Hello World}

The next example in figure~\ref{ex:helloworld2} creates 2 tasks that display two messages.
\begin{figure}[h]
\hrule\vspace*{2ex}
\begin{center}
\begin{minipage}{0.9\linewidth}
\begin{verbatim}
1  #include <stdio.h>
2  
3  #pragma kaapi task value(msg)
4  void say_hello(const char * msg)
5  {
6    printf("%s\n", msg);
7  }
8
9  int main(int argc, char** argv)
10 {
11 #pragma kaapi parallel
12   {
13     say_hello("First call: Hello World!");
14     say_hello("Second call: Hello World!");
15   } 
16   return 0;
17 }
\end{verbatim}
\end{minipage}
\end{center}
\hrule
\caption{hello\_world2.c example}\label{ex:helloworld2}
\end{figure}

After compilation and execution we hope to have such output:
\begin{verbatim}
> ./hello_world2
First call: Hello World!
Second call: Hello World!
\end{verbatim}

Moreover, some execution may produce following output:
\begin{verbatim}
> ./hello_world2
Second call: Hello World!
First call: Hello World!
\end{verbatim}

In \kaapi the task creation is a non blocking operation: the thread that creates tasks never wait the completion of the tasks if it is not specify by the user.  All tasks created into a parallel region are guaranteed to be completed at the end of the parallel region: the end of a parallel region has an implicit synchronization point that forces execution of tasks.  But inside a parallel region task may be executed in \textbf{any order} that respect the data flow dependencies between tasks. In the example, tasks have been declared as taking their parameter by value so they are independent.

The two ways \kaapi offers to enforce an execution order is:
\begin{itemize}
\item tasks have declared special access modes to theirs parameters (read, write, exclusive) which introduce read-after-write data flow dependencies between tasks: this is the only dependencies that \kaapi must respect at runtime. This is the subject of the next section.
\item if tasks are independent, then the user mays express a synchronization point using the \texttt{\#pragma kaapi sync} directive to force all tasks created before the synchronization point to be completed at the end of the synchronization point. In the previous example, this directive must be placed between line 13 and line 14.
\end{itemize}

Note that at the end of a parallel region there is an implicit synchronization point: to enforce order in the previous example, the user may also embedded created tasks into 2 parallel region. The code of figure~\ref{ex:helloworld2} will becomes:
\begin{verbatim}
11 #pragma kaapi parallel
12     say_hello("First call: Hello World!");
13 #pragma kaapi parallel
14     say_hello("Second call: Hello World!");
\end{verbatim}


\subsection{A simple writer-reader scheme}

The example for figure~\ref{ex:simpleWR} creates two tasks in a parallel region defined at line 18. The tasks are created at line 20 and 21.
The first task takes 3 parameters: the first one if the size of the buffer parameter, the second parameter. A third parameter points to the message to copy into the buffer. The respective access modes for this parameter are: \texttt{value} for the size; \texttt{write} for the buffer; \texttt{read} for the messsage. They are defined at line 3.

\begin{figure}[ht]
\hrule\vspace*{2ex}
\begin{center}
\begin{minipage}{0.9\linewidth}
\begin{verbatim}
1  #include <stdio.h>
2 
3  #pragma kaapi task write(buffer) value(size) read(msg)
4  void write_msg(int size, char* buffer, const char* msg)
5  {
6    snprintf(buffer, size, "%s", msg);
7  }
8 
9  #pragma kaapi task read(msg)
10 void print_msg(const char* msg)
11 {
12   printf("%s\n", msg);
13 }
14 
15 int main(int argc, char** argv)
16 {
17   char buffer[32];
18 #pragma kaapi parallel 
19   {
20     write_msg(32,buffer, "This is may be a too long \
           message for the buffer.");
21     print_msg(buffer);
22   }
23 
24   return 0;
25 }
\end{verbatim}
\end{minipage}
\end{center}
\hrule
\caption{simple\_prodcons.c example}\label{ex:simpleWR}
\end{figure}

The second task declare at line 9 that it read will read its single parameter.

The same effective parameter \texttt{buffer}, declared at line 17, is passed between the two tasks at line 20 and 21. Because of the previous declaration of \texttt{write} access in the first task and \texttt{read} access in the second task, it defines a \textbf{true} data flow dependency between the both tasks.

In \kaapi, any parameter that has to be shared between tasks must be a pointer to a data structure. In the previous example, it is a C-string, \textit{i.e.} a pointer to a null terminated array of characters.


\subsection{Recursive computation: Fibonacci}
The figure~\ref{ex:fibonacci} presents a recursive computation of the $n$-th Fibonacci number\footnote{For simplification, $n$ was fixed to 30 in the code.} using the classical non optimal algorithm.

\begin{figure}[ht]
\hrule\vspace*{2ex}
\begin{center}
\begin{minipage}{0.9\linewidth}
\begin{verbatim}
1  #include <stdio.h>
2 
3  #pragma kaapi task write(result) value(n)
4  void fibonacci(long* result, const long n)
5  {
6    if (n<2)
7      *result = n;
8    else 
9    {
10     long r1,r2;
11     fibonacci( &r1, n-1 );
12     fibonacci( &r2, n-2 );
13 #pragma kaapi sync
14     *result = r1 + r2;
15   }
16 }
17 
18 #pragma kaapi task read(result) 
19 void print_result( const long* result )
20 {
21   printf("Fibonacci(30)=%li\n", *result);
22 }
23 
24 int main()
25 {
26   long result;
27 #pragma kaapi parallel
28   {
29     fibonacci(&result, 30);
30     print_result(&result);
31   }
32   return 0;
33 }
\end{verbatim}
\end{minipage}
\end{center}
\hrule
\caption{fibonacci.c example}\label{ex:fibonacci}
\end{figure}

The main function creates two tasks: the first one to do the computation at line 29 and the second one to print the result (line 30).
Due to access mode, this two tasks have a true dependency. Like other example, these tasks are created and executed inside a parallel region.

The \verb+fibonacci+ task (lines 3-16) recursively creates two tasks until the input parameter \texttt{n} is less than \texttt{2}.
%%In \kaapi, tasks must be created into a parallel region: in \kaapi \textbf{each body of a task is an implicit parallel region}.

Task creation is a non blocking call, thus at line 13 a synchronization point is added in order to wait the completion of all the previous created tasks in the same parallel region, \textit{i.e.} completion of the two previously created tasks.

\subsubsection{Optimization}
The previous code of figure~\ref{ex:fibonacci} has a main drawback: it requires a synchronization at each step of the recursion. Even if, in the general case of execution, this synchronization does not wait because tasks are generally executed sequentially, it add some extra operations. 

In this section, we presents how to suppress it by creating a task that will represent the \textit{continuation} 
of the computation.
The code is presented in the figure~\ref{ex:fibonacci2}.
In place of adding a synchronization point, we create a task \texttt{sum}, with true data flow dependencies with the previous tasks. This \texttt{sum} task do the summation (task sum at line 18) of the two sub-results computed by the recursive tasks created at line 16 and 17.

\begin{figure}[ht]
\hrule\vspace*{2ex}
\begin{center}
\begin{minipage}{0.9\linewidth}
\begin{verbatim}
1  #pragma kaapi task write(result) read(r1,r2)
2  void sum( long* result, const long* r1, const long* r2)
3  {
4    *result = *r1 + *r2;
5  }
6 
7  #pragma kaapi task write(result) value(n)
8  void fibonacci(long* result, const long n)
9  {
10   if (n<2)
11     *result = n;
12   else 
13   {
14   #pragma kaapi taskscope data(r1,r2)
15     long r1,r2;
16     fibonacci( &r2, n-2 );
17     fibonacci( &r1, n-1 );
18     sum( result, &r1, &r2);
19   }
20 }
\end{verbatim}
\end{minipage}
\end{center}
\hrule
\caption{fibonacci2.c example.}\label{ex:fibonacci2}
\end{figure}

No more synchronization is required, but the user must take care about the scope of the variables  passed to effective  parameters to the tasks.
Creating a task is a non blocking operation, thus the 3 tasks at line 16, 17, 18 reference automatic variables (\texttt{r1, r2}) 
that will be invalid at the end of their scope: \textit{i.e.} after line 19. In order to set the scope of this variable within the scope of the created tasks, the user can specify it with the directive \texttt{\#pragma kaapi taskscope data} at line 14.

%\subsubsection{Terminal recursion suppression}
%
%The listing of figure~\ref{ex:fibonacci2} may also be optimized in order to avoid creation of task in the terminal recursive call at line 16. \kaapi provides a special clause to avoid to generate a task at each function call annotated to be task.
%The \texttt{\#pragma kaapi notask} directive only operates onto the next statement: it should be inserted between line 16 and line 17. 
%%\begin{figure}[ht]
%%\hrule\vspace*{2ex}
%%\begin{center}
%%\begin{minipage}{0.9\linewidth}
%%\begin{verbatim}
%%1  #pragma kaapi task write(result) value(n)
%%2  void fibonacci(long* result, const long n)
%%3  {
%%4    if (n<2)   *result = n;
%%6    else {
%%8    #pragma kaapi data alloca(r1,r2)
%%9      long r1,r2;
%%10     fibonacci( &r2, n-2 );
%%11   #pragma kaapi notask
%%12     fibonacci( &r1, n-1 );
%%13     sum( result, &r1, &r2);
%%14   }
%%15 }
%%\end{verbatim}
%%\end{minipage}
%%\end{center}
%%\hrule
%%\caption{fibonacci2.c example with suppression of terminal recursive call to Fibonacci.}\label{ex:fibonacci3}
%%\end{figure}
%
%The scope of the directive \texttt{\#pragma kaapi notask} is the call of the  next statement only.
%

\newpage\section{\kaapi programming model}

The task model used in this work comes from the Athapascan~\cite{Athapascan98} task model and semantics. As for Cilk, Intel TBB, OpenMP-3.0 or SMPSs the semantic remains sequential that ensure that the value returned by the read statements is the last written value according to the lexicographic order defined by the program: statements are lexicographically ordered by ';'. 
At runtime, the execution generates and executed a sequence of tasks that made access to the shared memory.

In the following sections, we outline the \kaapi programming model, then we specify different \kaapi constructions and thier semantics.

\subsection{Task execution model}

A \kaapi program is composed by C or C++ code and some annotations specified by a programmer to indicates what function to use to create tasks when they are called.
A \kaapi program begins by executing the C/C++ main entry point of the processus. Then, each time a call to an annotated function is encountered, a task is created and the control flow continue its execution until it reach a synchronization point.\\
 
A task is a function call without side effect: a function that should return no value except through the shared memory and the list of its effective parameters. It is to the responsibility to the programmer to annotate functions for which calls become tasks by using the \texttt{\#pragma kaapi task} directive. The parallelism in \kaapi is explicit (task annotation) while the detection of synchronizations is implicit: the dependencies between tasks and the memory transfers are automatically managed by the runtime.\\


A task implements a sequential computation whose granularity is fixed by the user; it is created in program statements that correspond to calls to functions annotated to be task thanks to the \texttt{\#pragma kaapi task} directive.
Tasks share data if they have access to the same memory region. A memory region is defined by a set of addresses in the virtual address space of the processes. This set has a shape: a multi-dimensional array.

The user is responsible to indicates the mode each task accessed to the memory: the main access modes are \textit{read}, \textit{write} or \textit{exclusive}.\\

One of the main characteristic to the \kaapi programming model is that it preserve the sequential semantics of the program: the result of the parallel execution remains the same as the sequential execution of the program compiler without the \kaapi compiler.


%%%
\subsection{Task}
\kaapi and previously Athapascan is called  macro data flow language: the user is  responsible to define granularity of the computation (task) as well as data shared between tasks. The runtime automatically schedule tasks in any order that respect the true data flow dependencies, \textit{i.e.} the data flow dependencies. All other dependencies are subject to be removed by renaming data.

\subsubsection{Task declaration}
The code of figure~\ref{fig:accummainkaapi} illustrates the definition of two tasks:
\begin{itemize}
\item The declaration of a task is a code annotation programs by \texttt{\#pragma kaapi task} of a function declaration with named formal parameter. 
A defining declaration is also possible.

\item The access mode of each formal parameter must be defined.
\end{itemize}

\begin{figure}[ht]
\hrule\vspace*{1ex}
\begin{center}
\begin{minipage}[t]{0.9\linewidth}
\begin{verbatim}
1  #pragma kaapi task value(n) read(array[n]) write(result) 
2  void accumulate (int n, int* array, int *result);

3  #pragma kaapi task read(result) 
4  void printresult( int* result );
\end{verbatim}
\end{minipage}
\end{center}
\vspace*{-2ex}
\hrule
\caption{Kaapi accumulation code with main code}
\label{fig:accummainkaapi}
\end{figure}

The task declared at line 1 declares the fact that the parameter \texttt{n} will be passed by value; the parameter \texttt{array} reference a 1-dimensional array of size \texttt{n} that will be read; and the parameter \texttt{result} will be write by the task to store a result.
This result could be typically read by a task that will take it with a \texttt{read} access mode, such as the task declared at line 4.

They are two kind of access mode for parameter:
\begin{itemize}
\item by value: a parameter is copied into the task's parameter during the creation and any modification done by  the task definition (task body) will never be visible outside the task execution
\item by reference: this access mode is used to declare input or output or input/output parameters of a task. In that case, it should be a pointer and a definition of the accessed memory region (multi-dimension array).
\end{itemize}

The section~\ref{sec:accessmode} presents all the different ways to declare access mode for each parameter.


\subsubsection{Task definition}
As for function, a task declaration represents the signature of a function. The task definition is the implementation of the code executed during task execution.

From the C/C++ programmer point of view it corresponds to the function definition of a task function declaration.
\begin{figure}[ht]
\hrule\vspace*{1ex}
\begin{center}
\begin{minipage}[t]{0.9\linewidth}
\begin{verbatim}
1  #pragma kaapi task value(n) read(array[n]) write(result) 
2  void accumulate (int n, int* array, int *result);

3  void accumulate (int n, int* array, int *result)
4  {  *result = std::accumulate(array, array+n ); }
\end{verbatim}
\end{minipage}
\end{center}
\vspace*{-2ex}
\hrule
\caption{Kaapi accumulation code with main code}
\label{fig:accummainkaapi}
\end{figure}
In figure 7, the line 1 and 2 correspond to the task declaration. The line 3 through 4 is the task definition.


\subsubsection{Task creation}
The creation of a task is simply represented by the call to the function annotated to be a task. The \kaapi replaces all function calls to tasks by the creation of a task structures that capture the effective parameters.

At the difference to the sequential call to function, the \kaapi task creation is a non blocking operation: the callee does not wait the completion of the task before to resume its execution. At runtime, the thread of control that generates tasks that will be executed in concurrence depending of the available resources. The execution flow of C/C++ code and the execution flow of tasks are not synchronized except implicitly on certain point, or on request to the programmer to synchronize them (see section~\ref{sec:synchro}).

Task creation may be part of the task definition, \textit{i.e.} at the difference of the StarSs programming model, \kaapi allows tasks to be created by task execution.
See section~\ref{sec:recursive} about recursive computation with~\kaapi.

A task must be created into a parallel region (see section~\ref{sec:parallelregion}). Outside a parallel region, any call to annotated task function is a standard sequential call to the function.


\subsection{Parallel region} \label{sec:parallelregion}
%\textit{TODO: revoir la sémantique de ces régions parallèles: 1/ nombre de threads 2/ définition emboîtée 3/ exécution séquentiel en dehors ou bien exécution par 1 thread ?}


A parallel region is annotated by the directive \texttt{\#pragma kaapi parallel} as at line 6 of figure~\ref{fig:accumparallelregion}. All tasks created inside a parallel region are executed with possible concurrency:
a parallel region is a dynamic scope where several threads cooperate to execute created tasks.
\begin{figure}[ht]
\hrule\vspace*{1ex}
\begin{center}
\begin{minipage}[t]{0.9\linewidth}
\begin{verbatim}
1  int main(int argc, char** argv)
2  {
3    int size;
4    int* A = ...;
5    int result;
6  #pragma kaapi parallel
7    accumulate (size, A, &result);
8    return 0;
9  }
\end{verbatim}
\end{minipage}
\end{center}
\vspace*{-1ex}
\hrule
\caption{Kaapi accumulation code with main code}
\label{fig:accumparallelregion}
\end{figure}

At the end of a parallel region, there exists an implicit synchronization point that wait for all previously created tasks.

\noindent \textit{\paragraph{TODO}: nowait clause ?}\\

Parallel regions may be embedded: the parallel regions share the same set of threads for concurrent execution.

Before the control flow encountered a parallel region, the execution is driven by the main thread.
Outside the outermost parallel region, only the main thread drives the execution. This does not imply that synchronization becomes unnecessary: task creation is always a non blocking instruction and synchronization may be required to synchronize the 
sequential execution flow with the task execution flow.

\subsubsection{Contextual parallel region definition}
The scope of a parallel region is the next instruction or the next basic block (between '{' and '}') if the directive is placed just before. For instance in figure~\ref{fig:accumparallelregion}, the declaration of the parallel region at line~6 only applies to the statement at line 7 that create a task.

\begin{verbatim}
#pragma kaapi parallel 
\end{verbatim}

The runtime select the number of threads using 1/ \kaapi environment variables \texttt{KAAPI\_CPUCOUNT} and \texttt{KAAPI\_CPUSET}, that define both the number of cores and a set of cores to used; 2/ else  \kaapi creates one thread per physical core of the machine.

%Any instruction that goes out of the parallel region (\texttt{return} instruction or \texttt{goto} instruction) 

%\subsubsection{Non contextual parallel region definition}
%It is also possible to mark a section to be a parallel section using \texttt{\#pragma kaapi parallel\_begin} and \texttt{\#pragma kaapi parallel\_end} directive:
%\begin{verbatim}
%#pragma kaapi parallel_begin
%... <code> ...
%#pragma kaapi parallel_end
%\end{verbatim}
%In that case, the begin and end directives of the section must appears in the same block in the C/C++ code. 
%


\subsubsection{Enclosed parallel region}
Parallel region may be defined inside parallel region. At the difference of OpenMP, a parallel region is not associated to a number of threads and the enclosed parallel region does not create another set of threads: the threads are shared among the region and the number of threads is controlled by \kaapi environment variables \texttt{KAAPI\_CPUCOUNT} and \texttt{KAAPI\_CPUSET} (see above).

The only guarantee is that before any parallel region and outside the outermost parallel region, the tasks are performed by the main thread.

\subsubsection{Implicit parallel region}
In special context, they are implicit parallel region. Currently the task definition (function body of a task definition) is a parallel region. Recursive calls to a task definition function creates tasks. See section~\ref{sec:recursive} about parallel recursive programming with \kaapi.

%%%%
\subsection{Access mode of tasks} \label{sec:accessmode}
 Two tasks sharing data have the same effective parameter, \textit{i.e.} a region into the shared memory. A task should declares how it accesses regions  in the shared memory.   
Several access modes have been defined since the first vesion Athapascan~\cite{Athapascan98}: \textbf{read}, \textbf{write}, \textbf{readwrite} or \textbf{exclusive} and \textbf{cumulative write}. Each of these access modes limits the kind of operation a task can made to a memory region. For instance, a task that declares an access mode read to one of its effective parameters cannot write value. These access modes have either a direct correspondence in StarSs\footnote{Called direction as in CORBA} or PLASMA; or are similar (reduction in StarSs).

%For instance, the StarSs\textbf{input} direction specification corresponds to the \textbf{read} access mode in \kaapi;  \textbf{output} direction corresponds to \textbf{write}; and \textbf{inout} corresponds to \textbf{readwrite}.  

Each formal parameter of a task should be attached to an access mode, else it is a compiler error.
The access mode is defined onto a memory region, \textit{i.e.} a set of addresses in the virtual address space of the processus.

In example~\ref{fig:accummainkaapi}, both \texttt{accumulate} and \texttt{printresult} defined a memory region pointed by the formal parameter \texttt{result}: the region is defined by 1 integer.
The region of the read parameter in task \texttt{accumulate} is an 1-D array of size specify in the \texttt{\#pragma kaapi task} clause. Any arithmetic expression (with cast) is possible to defined size of array. The value reference should correspond to the name of the formal parameter. 

The table~\ref{table:accessmode} resumes the different access modes:
\begin{table}[h]
\begin{center}
\small \footnotesize
\begin{tabular}{c|c|c}
Access mode      & ~Kaapi clause~ & ~StarSs clause~  \\
\whline
passing by value & \texttt{value(x)} & no such access \\ \hline
read access & \texttt{read(x)} or \texttt{r(x)} & \texttt{input(x)} \\ \hline
write access & \texttt{write(x)} or \texttt{w(x)} & \texttt{output(x)} \\ \hline
exclusive access & \texttt{readwrite(x)} or \texttt{rw(x)} & \texttt{inout(x)} \\ \hline
cumulative write & \texttt{reduction(op:x)} or \texttt{cw(op:x)} & \texttt{reduction(x)} \\ \hline
\end{tabular}
\end{center}
\caption{Different access modes in \kaapi. Equivalence with StarSs.}
\label{table:accessmode}
\end{table}

\subsubsection{Passing rule by value}
Any function parameter pass by value must be defined to be of access mode by value.
In that case, the effective parameter is copied at task's creation time and any modification remains local to the function.

Any C or C++ type may by defined to be passed by value.
The syntax is the following:
\begin{verbatim}
#pragma kaapi task value( list_declaration )
\end{verbatim}
\verb+list_declaration+ is a rule in the grammar given in the appendix~\ref{sec:grammar}.

\subsubsection{Passing rule by reference}
All description of memory region in \kaapi must be defined on C/C++ pointer type formal parameter. At runtime, the pointer, the declaration of the memory region and the access mode (read, write, ...) made by the task allow to detect dependencies.

The syntax of the clause is:
\begin{verbatim}
#pragma kaapi task mode ( list_declaration )
#pragma kaapi task mode_cw ( operator: list_declaration )
\end{verbatim}
The access mode name \texttt{mode} of the clause is given by table~\ref{table:accessmode}.
The previous example accumulate define 3 arguments. 2 of them are passed by reference: \texttt{array} is an array of size \texttt{n} (known at runtime); \texttt{result} is pointer to one integer.

\paragraph{1-D array.} They are two classical ways to specify a one dimensional array. The first one is by given a base address and a size:
\begin{verbatim}
#pragma kaapi task value(n) read(array[n]) write(result)
void accumulate( int n, int* array, int* result );
\end{verbatim}
The dimension of array must follows the subset of the C grammar for expression. Only reference to variable in the formal parameter list can be done.

The second is to specify a interval between two addresses (as used in the most of the STL algorithms). The syntax use the syntax for interval \texttt{[a:b]}, where \texttt{a} and \texttt{b} are identifiers of formal parameters.
\begin{verbatim}
#pragma kaapi task value(n) read( [begin:end] ) write(result)
void accumulate( int* begin, int* end, int* result );
\end{verbatim}
In the STL, the righ bound is not part of the interval and it is called the 'past the last' value of the iterator. The size of the interval is \texttt{b-a}. In C, \texttt{a} and \texttt{b} must be pointer to any type; in C++ they can also be random access iterators.


\paragraph{2-D array.} 2-D arrays appear de facto in linear algebra subroutines. In order to allows to split data structure in order to execution in parallel sub-computations, it  is necessary to handle sub sets of 2-D arrays.

\kaapi declaration follows idea of the BLAS storage for dense matrix. 
The syntax of the clause for 2-D array is:
\begin{verbatim}
#pragma kaapi task mode( array[n][m] )
#pragma kaapi task mode( array{ ld=identifier; [n][m] } )
#pragma kaapi task mode( array{ storage=C; \
                                ld=identifier ;  \
                                [n][m] } )
#pragma kaapi task mode( array{ storage=Fortran; \
                                ld=identifier ; \
                                [n][m] } )
\end{verbatim}
Where the \texttt{ld} must be a reference to a formal parameter name. The dimension must follows the sub set of the C grammar for expression. 
C or Fortran keyword are used to specify the storage of the matrix (row major or column major).

All this rules are defined in the grammar of the appendix~\ref{sec:grammar}.


%%%%
\subsubsection{Cumulative access mode}\label{sec:cw}
In \kaapi, it is possible to reduce variable with only associative operator. The commutativity is not required.
A task that required an accumulative semantic with respect to one of its formal parameters, must defined it with the \texttt{reduction}  clause in the \texttt{\#pragma kaapi task}. Two tasks having a reduction access mode onto the same memory region are concurrent. 
As depicted in figure~\ref{fig:exaccu},
\begin{figure}[ht]
\hrule\vspace*{1ex}
\begin{center}
\begin{minipage}[t]{0.97\linewidth}
%\scriptsize
\begin{verbatim}
#pragma kaapi task read(array[n]) reduction(+: result) value(n)
void accumulate (int n, int* array, int *result) 
{
   ...
}
\end{verbatim}
\end{minipage}
\end{center}
\vspace*{-1ex}
\hrule
\caption{Kaapi signature for accumulate with reduction}
\label{fig:exaccu}
\end{figure}
the reduction clause specifies: the operator to use during concurrent reduction and a list of  formal parameters on which the clause applied.
For all the scalar types of the C or C++ language, \kaapi predefines a list of known operators.
For each operator, the neutral element corresponds to the neutral element of the mathematical operator. The list is presented in table~\ref{table:opcumul} follows the same predefined reduction operators in OpenMP-3.0.
\begin{table}[h]
\begin{center}
\small \footnotesize
\begin{tabular}{c|c|c}
               & ~name~ & ~neutral~  \\
\whline
\verb!+! & addition & 0   \\  \hline
\verb!-! & subtraction & 0   \\  \hline
\verb!*! & multiplication & 1    \\  \hline
\verb!&! & bitwise and & \verb+~+0  \\   \hline
\verb!|! & bitwise or & 0  \\   \hline
\verb!^! & bitwise xor & 0  \\   \hline
\verb!&&! & logical and & 1 \\ \hline 
\verb!||! & logical or & 0 \\ \hline 
\end{tabular}
\end{center}
\caption{Predefined cumulative operators.}
\label{table:opcumul}
\end{table}

The user may have the availability to defined its own reduction operator using \texttt{\#pragma kaapi declare reduction} clause:
\begin{description}
\item [\texttt{\#pragma kaapi declare reduction}]~\\
\verb!(reduction-identifier : function-name)!\\
\verb![ identity( function-name) ]!\\
The \verb!function-name! must corresponds to the name of C/C++ function which must satisfies one of the following signature:
\vspace*{-2ex}\begin{verbatim}
  // For  C++
  void function( typename& result, const typename& value )
  // For C or C++
  void function( typename* result, const typename* value )
\end{verbatim}
\end{description}
The type of the parameter are inherited from the declaration of the reduction variable in the \texttt{reduction} clause to describe access mode of formal parameter.


In the same way, the identity function is used to initialized a value of a temporary variable to be the neutral of the reduction operator. The signature of the identity function must be one of the following:
\begin{verbatim}
  // For  C++
  void function( typename& value )
  // For C or C++
  void function( typename* value )
\end{verbatim}

%%%%
\subsubsection{Dependencies between tasks}
Once access modes are defined for the tasks, the runtime is able to detect dependencies between tasks that access to a same memory region.
\begin{description}
\item [Read after Write.] This is also called true dependency, it means that a task is reading a memory region produced by an other tasks.
\item [Write after Write.] This is a false dependency. Such dependency is removed using a renaming technique: the two tasks can be executed concurrently if they works on distinct memory region, thus the runtime may allocated distinct memory region.
\item [Write after Read.] This false dependency is also called anti-depedency. This kind of dependencies are subject to removal in the similar way as for write after write dependency.
\end{description}

Note that the \kaapi runtime used a lazy approach to remove false dependencies: if it is necessary to execute concurrently two tasks with false dependencies, then the runtime will rename the memory region. Else, the execution is sequential using the creation order between tasks.

In both case, the result of the execution remains the same whatever the execution order between tasks.


%%%%
%%%
\subsection{Directive for synchronization}\label{sec:synchro}

Because task creations that occur at each function call to task function are non blocking instruction, it may be
necessary to synchronize the control flow of the thread that creates tasks with the task executions themselves.

\subsubsection{\#pragma kaapi sync}

The directive forces the synchronization of the current executing control flow with all previously created tasks in the same task's activation frame.

For non recursive task's creations, this directive wait until all previously created tasks in the control flow have been completed.

For recursive task's creations, the directive wait until all previously created tasks, in the current task activation frame, has been completed.

\subsubsection{\#pragma kaapi sync(<list of views>)}

This directive forces the synchronization of the current executing control flow until the execution of the last created tasks, in the same task's activation frame, which write the memory describes by a the list of views.


%%%%
%%%
\subsection{Recursive function call}\label{sec:recursive}

\kaapi allows recursive task creations.
This is illustrated in figure~\ref{fig:recexaccu}, where the accumulate code recursively split the computation until a grain (of 2 here) by creating sub-tasks after splitting the array in two parts. The first created task will accumulate the values \texttt{array[0],...array[n/2-1]} and the second task will accumulate the values \texttt{array[n/2],...,array[n-1]}.
\begin{figure}[ht]
\hrule\vspace*{1ex}
\begin{center}
\begin{minipage}[t]{0.9\linewidth}
%\scriptsize
\begin{verbatim}
#pragma kaapi task read(n, array[n]) reduction(+: result) 
void accumulate (int n, int* array, int *result) 
{
   if (n < 2) *result += array[0] + array[1];
   else {
     accumulate (n/2, array, result);
     accumulate (n-n/2, array+n/2, result);
   }
}
\end{verbatim}
\end{minipage}
\end{center}
\hrule
\caption{Recursive Kaapi accumulate code}
\label{fig:recexaccu}
\end{figure}

This example is simple: no temporary data are required at each level of recursion. This not the case with the academic recursive non optimal computation of the fibonacci number.

\subsubsection{Control of the allocation scope of data}
All data passed by reference to tasks must be valid until the completion of the tasks.
For instance, because task creation is a non blocking call the example of figure~\ref{ex:caplante} is incorrect: the automatic variable \texttt{r} will be suppressed before the execution of the task \texttt{g}.
\begin{figure}[ht]
\hrule\vspace*{1ex}
\begin{center}
\begin{minipage}[t]{0.9\linewidth}
%\scriptsize
\begin{verbatim}
1  if (...)
2  {
3    int r; /* store sub result */
4    g(&r, input_values); /* create task*/
5  }
\end{verbatim}
\end{minipage}
\end{center}
\hrule
\caption{Incorrect code}
\label{ex:caplante}
\end{figure}

In that case, the user has two possibility:
\begin{itemize}
\item Add a synchronization point between line 4 and 5. That enforce the execution to wait the completion of \texttt{g} (left solution in figure~\ref{ex:caplanteplus}).
\item Allocate the data with the same scope as for tasks. In that case, the data is allocated with a scope enough to execution of task \texttt{g} (right solution in figure~\ref{ex:caplanteplus}).
\end{itemize}

\begin{figure}[ht]
\hrule\vspace*{1ex}
\begin{center}
\begin{minipage}[t]{0.49\linewidth}
%\scriptsize
\begin{verbatim}
1  if (...)
2  {
3    int r; 
4    g(&r, input_values); 
5    #pragma kaapi sync
6  }
\end{verbatim}
\end{minipage}\vrule~~
\begin{minipage}[t]{0.49\linewidth}
\begin{verbatim}
1  if (...)
2  {
3    #pragma kaapi data alloca(r)
4    int r; 
5    g(&r, input_values); 
6  }
\end{verbatim}
\end{minipage}
\end{center}
\hrule
\caption{Solution for incorrect code of figure~\ref{ex:caplante}}
\label{ex:caplanteplus}
\end{figure}

The syntax of the \texttt{\#pragma kaapi data alloca} is:
\begin{description}
\item [\texttt{\#pragma kaapi data alloca}] \verb!(<list of variables>)!\\
The \verb!<list of variables>! describes a list of automatic variables that hope to be used in task's creations.
\end{description}

The \kaapi name of the directive was chose with analogy of the \texttt{alloca} function of the C library stdlib.  With sequential C or C++ program, the developer may allocate data in the stack using the \texttt{alloca} function. In that case, the scope of allocation is the same as the activation frame where the call to \texttt{alloca} occured.
With \kaapi \texttt{\#pragma kaapi alloca}, the user can allocate data within the same scope as created tasks in the same activation frame.

%%%%
\subsubsection{Example with recursive task's creations}\label{sec:alloca}

%With sequential C or C++ program, the developer may allocate data in the stack using the \texttt{alloca} function. In that case, the scope of allocation is the same as the activation frame where the call to \texttt{alloca} occured.
%With \kaapi, the user may also allocate data within the same scope as created tasks in an activation frame.

To control the allocation scope of automatic variables allocated into the C stack of example in figure~\ref{fig:recaccumalloca}, the programmer has to use the directive \texttt{\#pragma data alloca}  at line 11.
\begin{figure}[ht]
\hrule\vspace*{1ex}
\begin{center}
\begin{minipage}[t]{0.9\linewidth}
%\scriptsize
\begin{verbatim}
1  #pragma kaapi task read(result1, result2) write(result) 
2  void sum (int* result, int * result1, int* result2) 
3  {
4    *result = *result1 + *result2;
5  }

6  #pragma kaapi task read(n, array[n]) write(result) 
7  void accumulate (int n, int* array, int *result) 
8  {
9     if (n < 2) *result += array[0] + array[1];
10    else {
11 #pragma kaapi data alloca(subresult1, subresult2);
12      int subresult1;
13      int subresult2;
14      accumulate (n/2, array, &subresult1);
15      accumulate (n-n/2, array+n/2, &subresult2);
16      sum( result, &subresult1, &subresult2 );
17    }
18 }
\end{verbatim}
\end{minipage}
\end{center}
\hrule
\caption{Accumulate recursive code where the allocation of \texttt{subresult1} and \texttt{subresult2} is 
specify to have the same scope of the spawned task using directive \texttt{\#pragma kaapi alloca}.}
\label{fig:recaccumalloca}
\end{figure}



%\subsection{Definition of complex memory region}
%%
%%The declaration of the list of formal parameters with their access mode is  called the \textbf{task' signature}. The task should also be associated to an entry point called \textbf{task body} which is the function to execution during task execution. A task may have several bodies~\cite{kaapi_europar} depending of the target architecture were to execute it. 
%\textit{TODO. L'implémentation ne correspond pas encore à ça: à voir l'intérêt réél}
%
%The listing of figure~\ref{fig:recaccumcomplex} presents how to define access mode for the field of a data structure.
%Here the data structure is the struct \texttt{myarray} which defines a size (int) and the data (array of int).
%When the user call the task, it is necessary to define type of each field. Here the fields are passed in \texttt{read} mode.
%\begin{figure}[ht]
%\hrule\vspace*{1ex}
%\begin{center}
%\begin{minipage}[t]{0.9\linewidth}
%%\scriptsize
%\begin{verbatim}
%/* array struct definition */
%typedef struct myarray {
%  int size;
%  int* data;
%} myarray;
%
%#pragma kaapi task value(array.size) read(array.data[array.size]) \
%                   reduction(+: result)
%void accumulate (myarray array, int *result) 
%{
%  int n = array.size;
%  if (n < 2) *result += array.data[0] + array.data[1];
%  else {
%    myarray subarray1 = { n/2, array.data };
%    myarray subarray2 = { n-n/2, array.data + n/2 };
%
%    accumulate ( subarray1, result);
%    accumulate ( subarray2, result);
%  }
%}
%
%int main( int argc, char** argv )
%{
%  /* create and initialize the array */
%  int result;
%  myarray array;
%  array.size = 100;
%  array.data = malloc( 100 * sizeof(int) );
%  for (int i=0; i<100; ++i)
%    array[i] = rand();
%    
%  /* spawn tasks */
%#pragma kaapi parallel
%  {
%    accumulate( array, &result );
%    printresult( &result );
%  }  
%  return 0;
%}
%\end{verbatim}
%\end{minipage}
%\end{center}
%\vspace*{-2ex}
%\caption{Accumulate recursive code with sub shared access mode definitions. The printresult is the task defined in figure~\ref{fig:accummainkaapi}.}
%\hrule
%\label{fig:recaccumcomplex}
%\end{figure}
%
%Note that in this version, because arguments are passed by value, it is not necessary to allocate them in a specific way.
%If the programer wants to pass a pointer to a structure, then the sub arrays used at each recursive call must be defined to be allocated in the same stack as the stack, like in code of figure~\ref{fig:recaccumalloca} by using the \texttt{\#pragma kaapi data alloca} directive.
%
%Because data structure may contains a lot of data field member, it is possible to define the access mode for the whole data structure and refining the access mode for more interesting mode. 
%
%
%

%%%
%%%
\newpage \section{Examples}\label{sec:exp}


\subsection{Fibonacci computation}

\subsubsection{With sync}

\begin{small}
\lstset{numbers=left, numberstyle=\tiny, stepnumber=1, numbersep=5pt}
\lstset{commentstyle=\color{blue}}
\lstset{language=C}
\begin{lstlisting}[frame=tb]
#include <stdio.h>

#pragma kaapi task write(result) value(n)
void fibonacci(long* result, const long n)
{
  if (n<2)
    *result = n;
  else 
  {
    long r1,r2;
    fibonacci( &r1, n-1 );
    fibonacci( &r2, n-2 );
#pragma kaapi sync
    *result = r1 + r2;
  }
}

#pragma kaapi task read(result) 
void print_result( const long* result )
{
  printf("Fibonacci(30)=%li\n", *result);
}

int main()
{
  long result;
#pragma kaapi parallel
  {
    fibonacci(&result, 30);
    print_result(&result);
  }
  return 0;
}
\end{lstlisting}
\end{small}

\subsubsection{Without sync}
\begin{small}
\lstset{numbers=left, numberstyle=\tiny, stepnumber=1, numbersep=5pt}
\lstset{commentstyle=\color{blue}}
\lstset{language=C}
\begin{lstlisting}[frame=tb]
#include <stdio.h>

#pragma kaapi task write(result) read(r1,r2)
void sum( long* result, const long* r1, const long* r2)
{
  *result = *r1 + *r2;
}

#pragma kaapi task write(result) value(n)
void fibonacci(long* result, const long n)
{
  if (n<2)
    *result = n;
  else 
  {
#pragma kaapi data alloca(r1,r2)
    long r1,r2;
    fibonacci( &r1, n-1 );
    fibonacci( &r2, n-2 );
    sum( result, &r1, &r2);
  }
}

#pragma kaapi task read(result) 
void print_result( const long* result )
{
  printf("Fibonacci(30)=%li\n", *result);
}

int main()
{
  long result;
#pragma kaapi parallel
  {
    fibonacci(&result, 30);
    print_result(&result);
  }
  return 0;
}
\end{lstlisting}
\end{small}

\subsubsection{With reduction variable}

\begin{small}
\lstset{numbers=left, numberstyle=\tiny, stepnumber=1, numbersep=5pt}
\lstset{commentstyle=\color{blue}}
\lstset{language=C}
\begin{lstlisting}[frame=tb]
#include <stdio.h>

#pragma kaapi task reduction(+:result) value(n)
void fibonacci(long* result, const long n)
{
  if (n<2)
    *result += n;
  else 
  {
    fibonacci( result, n-1 );
    fibonacci( result, n-2 );
  }
}

#pragma kaapi task read(result) 
void print_result( const long* result )
{
  printf("Fibonacci(30)=%li\n", *result);
}

int main()
{
  long result;
#pragma kaapi parallel
  {
    fibonacci(&result, 30);
    print_result(&result);
  }
  return 0;
}
\end{lstlisting}
\end{small}


\subsection{Cholesky factorization}
This example shows one of the main difference between StartSs programming model in its ability to view sub matrix of matrix without necessity to reallocate the user's data structure.\\

\begin{tiny}
\lstset{numbers=left, numberstyle=\tiny, stepnumber=1, numbersep=5pt}
%%\lstset{commentstyle=\textcolor{blue}}
\lstset{language=C}
\begin{lstlisting}[frame=tb]
#include <cblas.h> 
#include <clapack.h> 


#pragma kaapi task value(Order, Uplo, N, lda) readwrite(A{ld=lda; [N][N]})
int clapack_dpotrf(const enum ATLAS_ORDER Order, const enum ATLAS_UPLO Uplo,
                   const int N, double *A, const int lda);


#pragma kaapi task value(Order, Side, Uplo, TransA, Diag, M, N, alpha, lda, ldb) \
                   read(A{ld=lda; [N][N]}) readwrite(B{ld=ldb; [N][N]})
void cblas_dtrsm(const enum CBLAS_ORDER Order, const enum CBLAS_SIDE Side,
                 const enum CBLAS_UPLO Uplo, const enum CBLAS_TRANSPOSE TransA,
                 const enum CBLAS_DIAG Diag, const int M, const int N,
                 const double alpha, const double *A, const int lda,
                 double *B, const int ldb);

#pragma kaapi task value(Order, Uplo, Trans, N, K, alpha, lda, beta, ldc) \
                   read(A{ld=lda; [N][N]}) \
                   readwrite(C{ld=ldc; [N][N]})
void cblas_dsyrk(const enum CBLAS_ORDER Order, const enum CBLAS_UPLO Uplo,
                 const enum CBLAS_TRANSPOSE Trans, const int N, const int K,
                 const double alpha, const double *A, const int lda,
                 const double beta, double *C, const int ldc);

#pragma kaapi task value(Order, TransA, TransB, M, N, K, alpha, lda, ldb, beta, ldc) \
                   read(A{ld=lda; [M][K]}, B{ld=ldb; [K][N]}) \
                   readwrite(C{ld=ldc; [M][N]})
void cblas_dgemm(const enum CBLAS_ORDER Order, const enum CBLAS_TRANSPOSE TransA,
                 const enum CBLAS_TRANSPOSE TransB, const int M, const int N,
                 const int K, const double alpha, const double *A,
                 const int lda, const double *B, const int ldb,
                 const double beta, double *C, const int ldc);


// Block Cholesky factorization A <- L * L\^t
// Lower triangular matrix, with the diagonal, stores the Cholesky factor.
void Cholesky( double* A, int N, size_t blocsize )
{
  for (size_t k=0; k < N; k += blocsize)
  {
    clapack_dpotrf(
      CblasRowMajor, CblasLower, blocsize, &A[k*N+k], N
    );

    for (size_t m=k+blocsize; m < N; m += blocsize)
    {
      cblas_dtrsm
      (
        CblasRowMajor, CblasLeft, CblasLower, CblasNoTrans, CblasUnit,
        blocsize, blocsize, 1., &A[k*N+k], N, &A[m*N+k], N
      );
    }

    for (size_t m=k+blocsize; m < N; m += blocsize)
    {
      cblas_dsyrk
      (
        CblasRowMajor, CblasLower, CblasNoTrans,
        blocsize, blocsize, -1.0, &A[m*N+k], N, 1.0, &A[m*N+m], N
      );
      for (size_t n=k+blocsize; n < m; n += blocsize)
      {
        cblas_dgemm
        (
          CblasRowMajor, CblasNoTrans, CblasTrans,
          blocsize, blocsize, blocsize, -1.0, 
          &A[m*N+k], N, 
          &A[n*N+k], N, 
          1.0, 
          &A[m*N+n], N
        );
      }
    }
  }
#pragma kaapi sync
}


/* Main of the program
*/
void main( int argc, char** argv )
{
  // matrix dimension
  int N = 32;
  if (argc > 1)
    N = atoi(argv[1]);

  // block count
  int block_count = 2;
  if (argc > 2)
    block_count = atoi(argv[2]);
    
  size_t blocsize = N / block_count;

  double t0, t1;
  double* A = 0;
  if (0 != posix_memalign((void**)&A, 4096, N*N*sizeof(double)))
  {
    printf("Fatal Error. Cannot allocate matrice A, errno: %i\n", errno);
    return;
  }

  // Generate matrix A 
  generate_matrix(A, N);

  // Cholesky factorization of A 
  Cholesky(A, N, blocsize);

  free(A);
  
  return 0;
}
\end{lstlisting}
\end{tiny}

%%%\subsection{Merge Sort}

%%%\subsection{NQueens}

\subsection{2D Histogram}
\begin{tiny}
\lstset{numbers=left, numberstyle=\tiny, stepnumber=2, numbersep=5pt}
\lstset{commentstyle=\color{blue}}
\lstset{language=C}
\begin{lstlisting}[frame=tb]

/* some statically configured params
 */
#define CONFIG_DATA_LOG2 12
#define CONFIG_DATA_DIM (1 << CONFIG_DATA_LOG2)
#define CONFIG_BLOCK_DIM (CONFIG_DATA_DIM / 4)


/* histogram reduction
 */
#pragma kaapi declare			\
  reduction(hist_redop: reduce_hist)	\
  identity(init_hist)

static void init_hist(unsigned int* p)
{
  unsigned int i;
  for (i = 0; i < CONFIG_DATA_DIM; ++i, ++p) *p = 0;
}

static void reduce_hist(unsigned int* lhs, const unsigned int* rhs)
{
  unsigned int i;
  for (i = 0; i < CONFIG_DATA_DIM; ++i, ++lhs, ++rhs) *lhs += *rhs;
}


/* accumulate a given block histogram.
   reduction initialization and operator are specified
   in the task pragma clause.
 */
#pragma kaapi task			\
  value(dim, lda)			\
  read(data{ld = lda; [dim][dim]})	\
  reduction(hist_redop: hist)
static void accum_block_hist
(
 const unsigned int* data,
 unsigned int dim,
 unsigned int lda,
 unsigned int* hist
)
{
  unsigned int x, y;
  for (y = 0; y < dim; ++y, data += lda)
    for (x = 0; x < dim; ++x, ++data)
      ++hist[*data];
}

/* a task is spawned foreach block to accumulate
   the histogram. the reduction is handled by the
   xkaapi runtime.
 */
static void compute_hist
(
 const unsigned int* data,
 unsigned int dim,
 unsigned int* hist
)
{
  static const unsigned int lda = CONFIG_DATA_DIM;
  static const unsigned int block_size =
    CONFIG_BLOCK_DIM * CONFIG_BLOCK_DIM;

  const unsigned int block_count = dim / CONFIG_BLOCK_DIM;
  unsigned int i, j;

  init_hist(hist);

  /* foreach block, accumulate histogram */
  for (i = 0; i < block_count; ++i)
  {
    for (j = 0; j < block_count; ++j)
    {
      /* compute start of block */
      const unsigned int* const p =
      	data + i * block_size + j * CONFIG_BLOCK_DIM;
      accum_block_hist(p, CONFIG_BLOCK_DIM, lda, hist);
    }
  }
}

/* main */
int main(int ac, char** av)
{
  static unsigned int hist[CONFIG_DATA_DIM];
  unsigned int* const data = gen_data(CONFIG_DATA_DIM);
#pragma kaapi parallel
  compute_hist(data, CONFIG_DATA_DIM, hist);
  return 0;
}
\end{lstlisting}
\end{tiny}


\subsection{SMPSs examples}

SMPSs is the implementation of the StartSs programming model for multicore architecture.
SMPSs is developed by the Barcelona Supercomputing Center (\url{http://www.bsc.es}) and is freely available for download.

All examples from SMPSs are accepted by the KaCC compiler.
The user may have a look at \url{http://www.bsc.es/plantillaG.php?cat_id=385} to download SMPSs software that contains examples.

\kaapi programming model defines a set of directives which are a superset of the StartSs directives. The appendix~\ref{app:smpss} describes more in details the differences between \kaapi and StartSs/SMPSs.

%%%
%%%
\appendix 

\newpage  \section{C and C++ support}\label{secC++compatibility}


\newpage \section{Grammars}\label{sec:grammar}

In the following grammar, we assume terminal symbols:
\begin{verbatim}
  identifier := [a-zA-Z_][a-zA-Z_0-9]*
  integral := [0-9]+
\end{verbatim}

\subsection{Grammar for description of task's parameters}
The following grammar describes the rules to define access mode for each
parameter in the \verb+#pragma kaapi task+ compiler directive.
\begin{verbatim}
      list_param :=
          access_param
        | access_param list_param
      
      access_param :=
          mode '(' list_declaration ')
       |  mode_cw '(' redop ':' list_declaration ')
      
      list_declaration :=
          range_declaration
        | list_declaration ',' range_declaration
        
      range_declaration :=
          identifier
        | identifier dimension
        | identifier '{' complex_view '}
        | '[' identifier ':' identifier ']'
        | '[' identifier ':' identifier ')'
      
      complex_view :=
          element_view
        | complex_dimension ',' element_view
      
      element_view :=
          'storage' '=' storage_class
        | 'ld' '=' identifier
        | dimension

      mode :=
        | mode_w | mode_r | mode_x | mode_cw
      mode_w :=
          'write'     | 'w'  | 'output'
      mode_r :=
          'read'      | 'r'  | 'input'
      mode_x :=
          'exclusive' | 'x'  | 'inout'
      mode_cw :=
          'reduction' | 'cw'
      mode_v :=
          'value'     | 'v'
      
      redop :=
          '+' | '-' | '*' | '&' | '|' | '^' | '&&' | '||'
        | identifier

      storage_class :=
          'rowmajor'    | 'C' | 'C++'
        | 'columnmajor' | 'Fortran'      
\end{verbatim}
\subsubsection{Note on access mode}
Most of the different access modes (\verb+mode_xx :=+ rules) have three formats: long, short and a format compatible with the  SMPSs programming model. 
If the reduction operator for an \verb+reduction+ access mode is left unspecified, they the default operator is \verb'+' for all the scalar type (integral value and floating point value). 
For other instance of type or class that does not have the \verb'+' operator defined, then this is an compiler error. The default identity operator is: for scalar value (integral value and floating value) to set the value '0' which is the neutral element with respect to the associative default \verb'+' operator. For other type or class which have a defined \verb'+' operator, then the neutral is assumed to be set by calling the default constructor.

\subsubsection{Note on range declaration}

In the range declaration rule, the programmer has to opportunity to define the set of memory address that the task will access. 
The set of memory address aims at having any shape. This set serves to detect dependencies between tasks. The implementation has currently 2 limits:
\begin{enumerate}
\item The grammar limits the shape to continuous D dimensional arrays, 
with an implementation limit to 2D data structures (see next section about the grammar).
\item The detection of dependency between tasks use an special memory address of the range declaration which is named the \textbf{referent}. This address is the address of the identifier in case of the D-dimensional format. And for 1D definition using the format \verb+[identifier .. identifier]+ this is the memory address given by the value of the first identifier (a pointer).
\end{enumerate}

The grammar may be extended in the future in order to take into account more complex shape declaration.


\subsubsection{Dimension definition}
The dimension in range specification is described by the following grammar. Expression rules in the grammar is a subset of the C expression rules.
\begin{verbatim}
      dimension :=
          one_dimension 
        | one_dimension dimension

      one_dimension :=
          '[' expression ']'

      expression :=
        additive_expression

      additive_expression :=
          multiplicative_expression
        | additive_expression '+' multiplicative_expression
        | additive_expression '-' multiplicative_expression
        
      multiplicative_expression
          cast_expression
        | multiplicative_expression '*' cast_expression
        | multiplicative_expression '/' cast_expression
        | multiplicative_expression '%' cast_expression

      cast_expression
          unary_expression
        | '(' type_name ')' cast_expression

      unary_expression
          primary_expression
        | SIZEOF unary_expression
        | SIZEOF '(' type_name ')'
      
      primary_expression
          identifier
        | integral
        | '(' expression ')'
\end{verbatim}


\subsection{Grammar for reduction operator}
The compiler directive \verb+#pragma kaapi declare <reduction_declaration>+ follows the next grammar. This grammar may be extended to match the OpenMP directive to declare reduction operators using pseudo variable.
\begin{verbatim}
      reduction_declaration :=
          reduction_definition
        | reduction_definition identity_definition
        
       reduction_definition :=
        | 'reduction' '(' identifier ':'  function_name ')'

       identity_definition :=
          'identity'  '(' function_name  ')'
        | 'identity'  '(' '{' initializer-list '}' ')'
\end{verbatim}
The role of the identity declaration is to specify the initialization a variable with the value that corresponds to the neutral element with respect to the reduction operator. For instance: \texttt{0} for operator \texttt{+} over scalar type.
For user's defined type, this function should be defined. In C++, if the identity function is not defined, the C++ empty constructor is used to initialize this value. In C, the default identity function set to zero the data structure. 

The signature of the reduction function must be:
\begin{verbatim}
// For C or C++
void function_name( type_name* result, const type_name* value ); 

// For C++
void function_name( type_name& result, const type_name& value ); 
\end{verbatim}
The signature for the identity function follows the same rule:
\begin{verbatim}
// For C or C++
void identity_name( type_name* value ); 

// For C++
void identity_name( type_name& value ); 
\end{verbatim}


\subsection{Grammar for \texttt{data alloca} directive}
The following grammar is involved in the directive
\begin{verbatim}
#pragma kaapi data alloca <list of variables>
\end{verbatim}
\begin{verbatim}
      list_of_variables :=
          identifier
        | list_of_variables ',' identifier 
\end{verbatim}
This directive must be defined in a basic block before the definition of automatic variable.
This is a compiler error to pass a non automatic name (\textit{e.g.} global variable)  or to declare the pragma after the definition of the variable.


%%%
%%%
\newpage
\section{KaCC compiler and runtime options}\label{sec:compilo}

\subsection{Compilation process with KaCC}

The compilation process with KaCC of a file is the following:
\begin{enumerate}
\item Each \kaapi directive  that begins \texttt{\#pragma kaapi} is interpreted and the code is rewritten to call function of the \kaapi library. Invalid use of \kaapi directive and some warning are outputed.
\item The backend compiler (C or C++) is called on the rewritten code to generate an object file.
\item To create an executable, KaCC pass the \kaapi library to the linker.
\end{enumerate}


\subsection{KaCC options}
The KaCC compiler recognized all the options of Gcc compiler suite (gcc, g++) and add an extra option:
\begin{description}
\item [-\,-keep]: keep the intermediate files. For each file, 2 intermediate files are generated. The first one is prefixed by \texttt{kaapi-} and is the translation of the file with replacement of function call by task creation.
\end{description}


\subsection{Backend compiler selection}
The backend compiler is the default C or C++ compilers defined during the installation step (see options with \texttt{configure --help}). It could be changed by the environment variables \texttt{CC} and \texttt{CXX}.

\subsection{Runtime environment variables}
\begin{description}
\item [KAAPI\_CPUSET]: if defined, and if the operating system support it, then the variable specifies the set of core to use in \kaapi program. If no defined, the operating system decides to schedule \kaapi threads onto the cores.
The syntax is a list of core identifiers or a range of core identifiers:
\begin{verbatim}
KAAPI_CPUSET=1,2,6,8,21
KAAPI_CPUSET=0-11,20-31
\end{verbatim}
The identifier corresponds to system identifier for the core.
\item [KAAPI\_CPUCOUNT]: if defined, then the variable specifies the number of core to use. This number should be less than the number of cores specified with \texttt{KAAPI\_CPUSET}.
\end{description}


%%%
%%%
\newpage
\section{ SMPSs task model compatibility}\label{app:smpss}

A parallel  SMPSs program running on a multicore is composed of several threads sharing the address space of a process. The threads perform tasks which may shared values in the address space, that we call shared memory between threads.
A task is a function call: a function and the list of its effective parameters. 
In  SMPSs~\cite{smpss-manual} the definition of a task is based on the annotation of the C function definition. Figure~\ref{fig:accumulatesmpss} illustrates the methodology: the original sequential code \verb+accumulate+ is annotated to be a task if compiled with  SMPSs (\verb+#pragma css task+). Each formal parameters is described with the access the task do: \verb+input+ means that the task is required it before execution; \verb+output+ means that the task will produce it. And \verb+inout+ for the both. A depicted  in the code, a formal parameter may be defined with a size in order to identify the memory region accessed by the task.
\begin{figure}[ht]
\begin{center}
\hrule\vspace*{1ex}
\begin{minipage}[t]{0.9\linewidth}
%\scriptsize
\begin{verbatim}
#pragma css task input(n, array[n]) output(result) 
void accumulate (int n, int* array, int *result) 
{
  int sum = 0; 
  for (int i=0; i<n; ++i)
    sum = sum + array[i];
  *result = sum;
}
\end{verbatim}
\end{minipage}
\end{center}
\hrule
\vspace*{-1ex}
\caption{ SMPSs Accumulate code}
\label{fig:accumulatesmpss}
\end{figure}

All tasks are created asynchronously: the callee returns immediately after the task creation, before its execution.  SMPSs only guarantee that true data flow dependencies (read after write) are verified:  SMPSs ensures that a task that consume a value produced by an other task will be executed in order. Two tasks without direct or indirect data flow dependencies can be executed out of order. To ensure serialization,  SMPSs defines two kind of barriers: a global barrier that wait until all previously created tasks have been executed; a local barrier that wait until a list of values have been produced.

The SMPSs programming model must be initialized before (\texttt{\#pragma css init} / \texttt{\#pragama css finish}), and allows reduction with locking mechanism~\cite{smpss-manual}.


\subsection{Compatibility}

The \kaapi compiler is allowed to parse and execute correctly all the SMPSs directives without any change in the source code. Moreover, due to its extended syntax to define memory region, \kaapi can be used where SMPSs requires source code modification (for instance in all  inplace SMPSs factorization algorithms -LU, Cholesky, QR, ...-).

The semantic of reduction variable in \kaapi allows to avoid locking mechanism at the expense of a memory copy.



\newpage \bibliographystyle{splncs03}
\bibliography{rt-kaapi.bib}

\end{document}

\endinput
%%
%% End of file `squelette-rr.tex'.
